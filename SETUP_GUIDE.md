# ğŸš€ Complete Application Setup Guide

## Overview

This application consists of three parts:
1. **Backend (Laravel)** - Article CRUD API + Database
2. **Scraper Service (Node.js)** - Web scraping with Puppeteer
3. **Frontend (React)** - User interface

---

## ğŸ“‹ Prerequisites

- PHP 8.1+ with extensions: pdo_pgsql, mbstring, xml
- PostgreSQL 17
- Node.js 18+
- Composer

---

## ğŸ”§ Initial Setup (One-time)

### 1. Backend Setup

```powershell
cd backend

# Install dependencies
composer install

# Configure environment (already done)
# .env file contains:
# - DB_CONNECTION=pgsql
# - DB_PASSWORD=root
# - APP_KEY (generated)

# Run migrations
php artisan migrate

# Clear caches
php artisan config:clear
php artisan cache:clear
```

### 2. Scraper Service Setup

```powershell
cd llm-pipeline

# Install dependencies
npm install

# Environment variables in .env:
# LARAVEL_API_URL=http://localhost:8000/api
# SERP_API_KEY=your_key
# OPENAI_API_KEY=your_key
```

### 3. Frontend Setup

```powershell
cd frontend

# Install dependencies
npm install

# No env needed (uses default localhost:8000)
```

---

## â–¶ï¸ Running the Application

### Terminal 1: Backend API
```powershell
cd backend
php artisan serve
```
**Runs on:** http://localhost:8000

### Terminal 2: Scraper Service
```powershell
cd llm-pipeline
npm run scraper
```
**Runs on:** http://localhost:3000

### Terminal 3: Frontend
```powershell
cd frontend
npm run dev
```
**Runs on:** http://localhost:5173

---

## ğŸ¯ Using the Application

### Step 1: Scrape Articles

1. Open browser: http://localhost:5173
2. In **Scraper Control** panel:
   - Set number of articles (e.g., 5)
   - Keep default URL or change
   - Click **"ğŸš€ Start Scraping"**
3. Wait for success message
4. Articles appear automatically in grid below

### Step 2: View Articles

- Browse articles in responsive card grid
- Each card shows:
  - Title
  - Version badge (original/enhanced)
  - Status badge (published/draft)
  - Author and date
  - Content preview

### Step 3: Manage Articles

**View Details:**
- Click any article card
- Modal opens with full content
- See all metadata

**Download:**
- Click "â¬‡ï¸ Download" button
- Gets formatted .txt file

**Delete:**
- Click "ğŸ—‘ï¸ Delete" button
- Confirm in dialog
- Article removed

---

## ğŸ¤– LLM Enhancement Process

### Run Enhancement Pipeline

```powershell
cd llm-pipeline
npm start
```

**What it does:**
1. Fetches latest article from database
2. Searches Google for article title (via SerpAPI)
3. Scrapes top 2 reference articles
4. Sends to OpenAI for enhancement
5. Saves enhanced version to database (linked via parent_id)

**Requirements:**
- Articles must exist in database
- OPENAI_API_KEY in .env
- SERP_API_KEY in .env

---

## ğŸ“Š Application Flow

```
USER â†’ Frontend (localhost:5173)
         â†“
    Backend API (localhost:8000)
         â†“
    Scraper Service (localhost:3000)
         â†“
    PostgreSQL Database

Separate Process:
LLM Pipeline â†’ SerpAPI â†’ Puppeteer â†’ OpenAI â†’ Database
```

---

## ğŸŒ API Endpoints

### Backend (localhost:8000/api)

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/articles` | List all articles |
| GET | `/articles/{id}` | Get single article |
| POST | `/articles` | Create article |
| PUT | `/articles/{id}` | Update article |
| DELETE | `/articles/{id}` | Delete article |
| GET | `/articles/{id}/download` | Download as .txt |
| POST | `/articles/scrape` | Scrape BeyondChats |

### Scraper Service (localhost:3000)

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/scrape` | Scrape articles |
| GET | `/health` | Health check |

---

## ğŸ¨ Frontend Features

### Control Panel
- Configure scraping parameters
- Start/stop scraping
- Real-time status updates

### Article Grid
- Responsive card layout
- Version & status badges
- Quick actions (download, delete)
- Click to view details

### Article Modal
- Full article content
- Complete metadata
- Clean formatting
- Close on overlay click

---

## ğŸ› ï¸ Troubleshooting

### Backend not starting
```powershell
cd backend
php artisan config:clear
php artisan cache:clear
```

### Database connection error
- Check PostgreSQL is running
- Verify password in `.env` (DB_PASSWORD=root)

### Frontend not loading
```powershell
cd frontend
rm -rf node_modules
npm install
```

### Scraper service failing
- Check if port 3000 is available
- Verify Puppeteer installed correctly
- Try: `npm install puppeteer --force`

---

## ğŸ“ Project Structure

```
beyondchats-assignment-full/
â”œâ”€â”€ backend/              # Laravel API
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Http/Controllers/ArticleController.php
â”‚   â”‚   â”œâ”€â”€ Models/Article.php
â”‚   â”‚   â””â”€â”€ Services/ArticleService.php
â”‚   â”œâ”€â”€ database/migrations/
â”‚   â”œâ”€â”€ routes/api.php
â”‚   â””â”€â”€ .env
â”œâ”€â”€ llm-pipeline/         # Node.js services
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/scraper.api.js
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ googleSearch.service.js
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.service.js
â”‚   â”‚   â”‚   â””â”€â”€ scraper.service.js
â”‚   â”‚   â””â”€â”€ workflows/articleEnhancer.workflow.js
â”‚   â””â”€â”€ .env
â””â”€â”€ frontend/             # React UI
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ api/article.api.js
    â”‚   â”œâ”€â”€ components/
    â”‚   â””â”€â”€ App.jsx
    â””â”€â”€ package.json
```

---

## ğŸ¯ Quick Start (All in One)

```powershell
# Terminal 1 - Backend
cd backend; php artisan serve

# Terminal 2 - Scraper
cd llm-pipeline; npm run scraper

# Terminal 3 - Frontend
cd frontend; npm run dev

# Then open: http://localhost:5173
```

---

## âœ… Checklist

- [ ] PostgreSQL running (service: postgresql-x64-17)
- [ ] Backend .env configured (DB_PASSWORD=root)
- [ ] Migrations run (articles table exists)
- [ ] Backend API running (port 8000)
- [ ] Scraper service running (port 3000)
- [ ] Frontend running (port 5173)
- [ ] Can scrape articles via UI
- [ ] Can view article details
- [ ] Can download/delete articles

---

## ğŸ“ Notes

- **Original articles**: `version: 'original'`, no parent_id
- **Enhanced articles**: `version: 'enhanced'`, has parent_id
- All articles stored in same `articles` table
- Soft deletes enabled (deleted_at column)
- Cache cleared automatically on modifications
